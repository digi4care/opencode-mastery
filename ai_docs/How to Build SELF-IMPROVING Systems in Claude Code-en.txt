[0.1s] What if your AI systems could improve
[2.1s] themselves? Not metaphorically, but
[4.4s] literally. Imagine a chatbot that can
[6.6s] detect its own bad responses and fix its
[9.2s] own prompts. Or an automation that runs
[11.5s] and breaks and can detect that
[13.3s] something's wrong and repair itself.
[15.2s] These kinds of concepts used to be
[17.0s] science fiction. But now you can build
[19.8s] them yourself. And I'll prove it to you
[21.4s] because I built one in Clawed Code and
[23.7s] I'm going to show you how you can do it,
[25.1s] too. And I've been able to use this
[26.6s] concept I'm about to show you in a
[28.7s] variety of different apps with different
[30.6s] applications. Because once you see this
[32.6s] pattern, you'll never build an AI app
[34.5s] the same ever again. Let's dive in. So
[36.6s] to keep this concept as approachable and
[38.8s] accessible to everyone as possible,
[40.9s] we're going to use this app that I put
[42.3s] together, which is just a chatbot. But
[44.2s] technically, it's not just any other
[45.9s] chatbot because like you can see on the
[48.4s] top right hand side, it's a chatbot that
[51.0s] retrains itself based on the
[52.9s] conversations that it has. So whether or
[55.0s] not a user said, "You're too dry. This
[57.6s] answer's way too verbose, this answer is
[59.8s] way too vague, it's way too AI," it can
[62.3s] detect and read through conversations
[64.6s] and decide based on a rubric that I've
[66.7s] created for it when it makes sense to
[68.9s] update its own prompt. Now, if you don't
[70.6s] believe me, we'll go into the admin tab
[72.9s] right here, and you'll see that we've
[74.7s] had a total of 60 messages, and we're
[77.6s] currently at the fourth version of the
[79.7s] current prompt. I've yet to intervene
[82.0s] myself on the system prompt. This is the
[84.2s] AI with basically another AI being a
[86.7s] judge reading through the conversations
[89.2s] and deciding what should change and why
[91.4s] it should change. And if we scroll down,
[93.2s] you'll see that we have a concept of
[94.8s] reflection here. And the way we're using
[97.1s] reflection is it will go and check the
[99.8s] last x number of messages. So it could
[102.2s] be the last seven exchanges or 14
[104.7s] messages, the last 10 exchanges or 20
[107.4s] messages. And then we can decide whether
[109.8s] or not we want the chatbot to look at
[111.8s] only the messages it has not looked at
[113.9s] before or if you want to actually change
[116.2s] the logic of the judge itself. We can
[118.5s] make it so that it consistently goes and
[120.5s] looks at the last n number of messages.
[122.6s] Now, if I'm already losing you, just
[124.0s] stick with me. I have diagrams. I have
[125.9s] visuals. I really want you to get this
[127.7s] cuz it's super cool. The TLDDR is I can
[129.9s] set this app to look every hour, every
[132.4s] 30 minutes, or we can make it every
[134.3s] couple hours to look through all of the
[136.5s] chats and decide is this chatbot still
[139.0s] on track, or have we had enough
[141.0s] conversations where it's clear that the
[143.0s] users are not being well serviced by it
[145.0s] and things need to be updated. So, if we
[147.0s] take a look at the last reflection right
[148.6s] here, you'll see it graded itself two
[150.7s] out of five on completeness, four out of
[153.2s] five on depth, four out of five on tone,
[155.9s] and one out of five on scope. [snorts]
[157.9s] So, as a result of that, it actually
[159.9s] shows you the conversation that was
[162.2s] included in this evaluation. So, you can
[164.4s] have a second set of eyes if you want to
[166.2s] see if your judge is doing the right
[168.6s] job. And if you want to see the analysis
[170.6s] of why it decided that it needed to
[172.8s] change its own prompt, it walks through
[174.9s] and breaks down in plain English why it
[177.7s] decided from what conversations that it
[179.8s] needed that trigger. But wait, there's
[181.6s] more. In the app itself, as we go from
[184.2s] version to version, if we decide as the
[186.6s] human that even though the AI decided
[188.5s] the system prompt should change, that we
[190.4s] want to revert back to a previous
[191.7s] version, we can always go back and
[193.5s] revert to whatever it is we had before.
[195.7s] And on top of that, if you want to audit
[197.4s] all the reflections, you'll notice that
[199.2s] the majority of the time that I ran
[200.8s] this, it passed, meaning it maintained
[203.5s] the system prompt. And usually a system
[205.8s] prompt for a chatbot shouldn't be very
[207.8s] reactive. I literally wrote a prompt so
[210.4s] good by accident that I had to manually
[212.7s] break it by making the rubric impossible
[215.7s] to pass. And since I had a database
[217.9s] behind the scenes, we could click on
[219.6s] anything like show right here. And we
[221.6s] can always go back in time to see why a
[224.2s] particular phase either failed or
[226.2s] passed. And the last tab we have is the
[228.2s] suggestions tab. And this is where the
[230.6s] LM as a judge gives any form of feedback
[233.4s] or tidbits that you could use to improve
[235.8s] the conversations based on what it's
[237.8s] seen, but it isn't enough evidence to
[240.0s] want to change the underlying system
[241.9s] prompts. So, for example, if we go on
[243.9s] this one, which says use more natural
[246.2s] conversational language, you can go
[248.2s] through and it says the assistance
[249.7s] response style is very structured and
[252.2s] professional throughout. While
[254.2s] appropriate for the technical audience,
[256.0s] this becomes slightly cold when a user
[258.6s] is in genuine distress. So imagine you
[261.1s] have a little Jira board or or a project
[263.2s] manager, an AI project manager. It can
[265.8s] go through and give you advice on how
[267.7s] you can improve without overriding what
[269.8s] you already have. So if I've piqu your
[271.1s] interest so far, let's take a look at
[273.0s] how we built it. Now, big picture.
[276.0s] Usually, especially in the world of Vibe
[278.1s] coding, many apps are built very
[280.4s] linearly where you write a prompt, you
[282.6s] pray it works, you fight with the AI,
[285.0s] you test it, you tweak it, you go back
[287.2s] and forth in this iterative circle. And
[289.3s] even when it's in production, even if
[291.0s] you get to that point where it's ready,
[293.1s] every single time you have an
[294.4s] interaction, you'll go through all the
[296.8s] interactions back and forth to see is it
[299.3s] performing up to par? And then at some
[301.4s] point you'll have a threshold or a user
[303.3s] swearing at you where you decide, okay,
[305.8s] now we have to make a change. This new
[307.5s] world I'm presenting you is a
[308.9s] self-improvement feedback loop where you
[311.8s] create the app in a way where you have
[313.8s] different parts of your database
[315.8s] tracking different pieces of metadata,
[318.2s] data in general, flow of the app, so it
[321.2s] can come up proactively with
[323.0s] suggestions. And if you want it to
[324.6s] implement such suggestions, then it can.
[326.9s] And if you've watched this far and
[328.6s] you're non-technical and you're sweating
[330.5s] already because you can imagine there's
[331.8s] so much technology at play. There's
[333.8s] literally two things I used to build
[335.4s] this one is Cloud Code. That's the
[338.1s] obvious one. The second one is
[339.9s] Superbase, a database. And all I did was
[342.4s] hook up the Superbase MCP server. And
[345.4s] this allows cloud code to go free reign,
[348.3s] build a new database, build whatever
[350.4s] functions need to be built, test them,
[352.9s] create new tables, and allow things like
[355.0s] edge functions, which help us create
[357.3s] micro interactions and behaviors
[359.0s] throughout the whole app. And the best
[360.7s] part is, despite my technical
[362.3s] background, all I did to build this was
[364.4s] use natural language prompts. So now to
[366.5s] get more granular, we'll go into teacher
[368.4s] mode and walk through the exact process
[370.6s] of how this is built. So you have cloud
[372.9s] code that builds and connects
[375.1s] everything. That's step one. Step two is
[377.4s] you want to be able to connect Superbase
[379.4s] via MCP server. This makes it a lot
[382.2s] easier to have cloud code go from a plan
[384.6s] to going back and forth and having a
[386.3s] feedback loop with Superbase. So if
[388.3s] something's failing, you don't have to
[389.8s] go back and forth and copy paste errors
[391.8s] and screenshot and tell cloud code about
[394.2s] it. There is this seamless
[395.9s] communication. It's like an open phone
[397.7s] line between both services. In
[399.5s] Superbase, we're going to do things like
[401.0s] store the prompts, the responses,
[403.7s] timestamps, timestamps of the last time
[405.9s] we ran a reflection, the logs associated
[408.9s] with that reflection, the prompts
[411.0s] associated with our rubric that should
[413.0s] persist over time, and the user prompts
[415.6s] that we change from time to time. So,
[417.4s] the biggest part of this process is
[418.7s] really ideulating what are all the
[421.0s] components you need to have an effective
[423.0s] feedback loop system. Now, the next
[424.7s] layer is the most important layer, which
[426.1s] is the evaluation layer. How do you
[428.4s] create a way where the AI can go through
[430.4s] some self assessment? Now, in this case,
[433.1s] we have the AI taking care of the chat
[435.7s] and we have another virginized blank
[438.2s] slate AI whose sole role is to monitor
[441.8s] the other AI's behavior and basically
[444.0s] give feedback on. So, as an analogy, if
[446.0s] you've ever had a 9to-5, you'd be
[448.2s] familiar with performance evaluations.
[450.5s] And typically, an employer would
[451.9s] evaluate you and once in a while you
[454.6s] would be asked to evaluate yourself. In
[456.6s] this case, we care about the latter
[458.1s] where we're giving a rubric where the AI
[460.8s] assesses itself and if it comes to the
[462.9s] honest conclusion without ego that it's
[465.7s] done poorly, then it tries to come up
[467.4s] with a plan on how it can improve. And
[469.0s] the key thing is that this is a feedback
[470.6s] loop. This can keep going on and on,
[473.1s] especially as you have more
[474.5s] conversations or more users in this
[476.5s] case. And to get even more granular, you
[478.5s] have the user ask a question and then
[480.6s] the chatbot responds and then if there's
[483.2s] a trigger to evaluate it, it will look
[485.6s] at the back and forth exchanges and then
[487.7s] it will score and save this to a
[489.4s] database. If nothing needs to be
[491.0s] changed, which should be the status quo,
[493.0s] you should not have a system prompt
[494.6s] changing non-stop. Otherwise, you're
[496.2s] going to have a very unstable app that
[498.6s] is very reactive to nuanced
[500.4s] conversations. If it decides that it
[502.3s] needs to update, then it will
[503.5s] automatically update and it will keep
[505.4s] going in this circle. And if it's not
[506.8s] clear by now, the main benefits is you
[508.8s] essentially have AI metarrompting
[511.2s] itself, using AI to write and evaluate
[513.9s] its own prompts, which is usually a much
[516.4s] better prompt engineer than you and I.
[518.0s] And not only that, it does give some
[519.6s] more richness to you as the owner of
[521.8s] this application cuz now you can see the
[524.0s] AI's thoughts, you can audit, and you
[526.1s] basically have a thought partner as to
[527.8s] how you can improve the experience of
[529.5s] your users on the app. Now, building the
[531.4s] system is doable, but it needs some
[533.4s] imagination and some back and forth. So,
[535.6s] while I still have maintained all my
[537.7s] chats where I'll pigeon hole certain
[539.8s] parts to show you to give you that
[541.5s] inspiration of how you could build
[543.0s] something just like this or apply this
[545.4s] concept elsewhere, I'll first walk you
[547.6s] through the entire journey of how we
[549.3s] went from beginning to end and I'll show
[551.8s] you the mega prompt that I started out
[553.7s] with. So, this was built over multiple
[555.6s] sessions. And in the first session, like
[557.8s] most sessions, you want to build the
[559.4s] foundation where you go through the big
[561.1s] picture and try to explain to it, you
[562.9s] know, this is the MCP server that you
[564.7s] want to connect to. This is Superbase.
[566.6s] You're going to use that primarily.
[568.2s] We're going to create different tables.
[569.8s] The goal is to create an experience
[571.6s] analyzer, and then we want to be able to
[573.6s] interchangeably change things in the
[575.4s] database, add new edge functions as
[578.2s] needed. So, I gave Cloud Code free reign
[580.5s] on the first pass to build as much as it
[582.9s] wanted and think about any thoughtful
[584.8s] features that made sense for this
[586.6s] self-improving system. By session number
[589.5s] two, I noticed it created its own
[591.6s] features like a cooldown feature where
[593.9s] once the prompt was updated, it would
[596.6s] prevent it would have a blackout period
[598.6s] where the prompt couldn't self-update
[600.6s] again until 1 hour. Engineering wise, it
[603.0s] actually makes a lot of sense, but for
[604.7s] my purposes of testing, I had to find a
[607.0s] way to break it. So in session three, we
[608.9s] focused on safety nets. How do we manage
[611.8s] the grading? How do I make sure that
[613.8s] it's not just being nice to itself every
[615.7s] single time it runs a self- assessment?
[617.6s] Like a human sometimes can be when
[619.8s] they're looking for a promotion or to
[621.4s] keep their job. Where in the self
[623.0s] assessment, even if they're not that
[624.5s] great of an employee, they might say, "I
[626.4s] am awesome. I am a 10 out of 10." And in
[628.6s] the other sessions like session four and
[630.7s] onwards, I created this handoff file
[633.5s] where because my conversations were
[635.3s] getting meaty very quickly, I tried to
[637.8s] create this baton pass method where I
[640.1s] could collapse a conversation and where
[642.1s] I could pass it off to the next agent to
[645.0s] continue and go from there. Now, brace
[647.0s] yourself and take a deep breath because
[649.0s] I'm about to spend the next few minutes
[650.9s] breaking down this mega prompt. And
[653.0s] we're not going to read it line by line,
[654.9s] but I'll give you enough of an idea that
[656.6s] you can appreciate what I used as a
[659.0s] foundation for all of this. And don't
[660.4s] worry about having to screenshot to keep
[662.2s] up. I'll make this available to you
[664.0s] along with some other goodies in the
[665.9s] second link in the description below.
[667.4s] So, let's get to it. We start off by
[669.3s] saying, "Build a self-improving chatbot
[671.8s] that answers questions about an AI
[673.8s] consultancy and business AI
[675.6s] transformation. The system uses
[677.5s] Superbase for persistence, fancy word
[679.9s] for keeping things in memory, aka a
[682.3s] database, and Claude 4.5 Haiku, cheap,
[685.5s] fast, easy to use for both the chat
[687.9s] agent and the self-improvement
[689.7s] reflection loop. Now, this is the part
[691.3s] where I had AI come up with a text
[693.0s] stack, a series of ideas for cloud code.
[695.4s] So, one of the most important things is
[696.9s] just feeding it 4.5 haiku and what the
[699.6s] model name is because most of these
[701.6s] models, even if they're trained as of
[703.2s] January 2025, still wouldn't know about
[706.1s] the existence of models like 4.5 haiku.
[708.5s] And you'll notice in things like cursor,
[710.3s] claude code, whatever it is you use,
[712.6s] they will always default to an old
[714.3s] version. So, if you ask for use Gemini,
[716.8s] it'll say Gemini 1.5 or Gemini 2. Same
[719.9s] thing with claude, it will use Claude
[721.5s] 3.5. And next up, I say I have superbase
[724.1s] MCP enabled. It's helpful before you
[726.4s] even start this session that you would
[728.1s] create and enable the MCP server so it's
[730.6s] connected and ready to go. And this is
[732.2s] the part where I really wanted the AI to
[734.2s] step in and do the heavy lifting. So it
[736.4s] created the database design. So it
[738.6s] created a table for users where it
[741.0s] collected information like ID, email,
[743.4s] created at data of the messages which is
[745.4s] super important because we need to be
[747.1s] able to track either X amount of
[749.2s] messages over X amount of time horizon.
[751.2s] And then authentication
[753.9s] [snorts] sessions. So multiple chat
[755.9s] sessions should be persisted. The number
[758.2s] of messages we had a messages table. And
[760.7s] then it created other tables like one
[762.2s] for system prompts. That's a no-brainer.
[764.1s] We have to store those over time.
[766.2s] Reflection log. So where we're going to
[768.5s] store the reflections of the AI judge,
[771.4s] any decisions. And the next part is the
[773.7s] setup of its own prompt using AI to
[776.3s] create that prompt. So meta prompting on
[779.1s] steroids. And this says, "Create a
[781.5s] well-crafted initial system prompt for
[783.7s] the AI consultancy chatbot. It should a
[787.0s] be an expert on AI consultancy, know
[789.9s] about common frameworks, understand
[791.9s] business context, and a series of other
[794.1s] piece of information. And this is
[795.5s] possibly the most important part where I
[797.5s] had Claude nerd out on what edge
[799.7s] functions might be needed to enable
[801.6s] different behaviors in the app. So we
[803.9s] have a chat handler that is responsible
[806.3s] for receiving the messages fetching the
[808.6s] conversations calling the API because
[811.6s] anthropics API is best called using
[814.0s] something like an edge function the
[815.5s] reflection loop how that would work. So
[817.6s] step one fetch the recent messages or
[819.9s] exchanges. Step two analyze with the
[822.6s] rubric that AI would help create the
[824.5s] first draft of and we told it create
[827.0s] some criteria from one to five. So
[829.0s] response completeness, response depth,
[831.5s] tone appropriateness, scope adherence,
[834.4s] missed opportunities. Step three was the
[836.7s] decision framework it would use to do
[838.6s] so. And then step four was when to
[841.7s] decide to update and what the criteria
[843.7s] for that is. And the rest of this looks
[845.7s] at things like the guardrails of the app
[847.7s] itself, how it should evaluate itself,
[850.3s] examples of reflection logs, so it can
[852.5s] basically model after those examples.
[854.6s] And then we enter this and go from
[856.0s] there. Now, instead of me going through
[857.6s] back and forth between two different
[859.1s] screens to show you snippets of chats,
[861.7s] I'll walk through the general concepts
[863.4s] of what needed to happen in each session
[865.9s] that was in my cloud code instance. And
[867.7s] what I'll also do is go back and forth
[869.6s] between the app so I can tell you
[871.4s] exactly where this issue was and how I
[873.7s] decided that we needed to iterate on it.
[875.4s] So this first session, like I said, was
[877.3s] all about foundations. So if I show you
[879.4s] a little teaser here, this MCP superbase
[882.6s] execute SQL. This is Superbase MCP at
[885.6s] its finest where it doesn't have to ask
[887.6s] me permission to write every single SQL
[889.7s] query. And it's very similar to an
[891.5s] experience you would imagine from
[893.3s] Lovable or Bolt or any one of those
[895.2s] other tools that are browserbased where
[897.0s] it would ask you permission to execute a
[898.8s] bunch of SQL which even if you have no
[901.0s] idea what it said, it would still
[903.0s] necessitate you to click on and enable
[905.1s] that. In this case, if you're being
[906.4s] experimental and you're going on YOLO
[908.4s] mode on bypass permissions, then this
[910.5s] will keep running, keep executing
[912.5s] different queries, testing those
[914.2s] queries, and this is where you can even
[916.0s] start to apply different features. You
[918.0s] can have multiple agents working on the
[920.2s] same task, each one working on a
[922.4s] different part of the database. So, the
[924.0s] most common error that popped up was I
[925.7s] initially wanted a way to connect a user
[928.5s] to a chat. So, if we go back to the app
[930.8s] itself and I am to log out, I want to
[934.0s] just be able to enter my name, enter my
[937.7s] email, click on start chatting, and it
[939.8s] would remember me from that email as a
[941.9s] unique identifier. And now, if you go
[943.7s] in, you'll see I have all my
[945.0s] conversations. So, we had some initial
[947.0s] problems there. And then the second part
[948.9s] was just establishing the connection to
[950.6s] Superbase, making sure it was working
[952.9s] and making sure that it wasn't going off
[954.8s] the rails. Now, when I initially set
[956.4s] this up, I got a screen where if I
[958.5s] clicked on a new chat and sent a
[960.2s] message, one, I couldn't see the message
[962.6s] that I sent, let alone the fact that
[964.6s] when we received a response, it would
[966.2s] kick me out of the chat. So, many times
[968.1s] when you're vibe coding, you'll have
[969.6s] these unexpected micro behaviors that
[971.9s] happen that you have to account for.
[973.4s] Another thing that I wanted was I wanted
[975.1s] to have some prompts here that give you
[977.1s] suggestions on what you could ask so
[979.0s] that when you click on it, it would also
[980.9s] send that prompt directly. So if I click
[982.6s] on what should I cover in an AI
[984.6s] discovery workshop, this should go. It
[986.6s] has a little loading state and comes
[988.5s] back with a response. The next thing is
[990.9s] the response came back with a series of
[992.6s] hashtags basically markdown. And we
[995.2s] needed a way that the AI could render
[997.5s] this. So it looks something like this
[999.0s] where it's well put together. It's
[1000.8s] structured. It's easy to read. So just
[1002.7s] this scope took us to the end of the
[1004.6s] context window of our first session. And
[1006.8s] then we moved on to part two. Part two
[1008.6s] is where we had a system prompt and I
[1010.7s] wanted to see nothing broke. It seems
[1012.9s] like it's functional. It looks like it's
[1014.4s] actually looking at the past chats, but
[1016.9s] I can't tell if it's actually working
[1018.3s] because it just says pass. So then I
[1020.5s] tried to manipulate the rubric. It came
[1022.4s] up with itself and realized like I
[1024.4s] mentioned, it came up with what's called
[1025.9s] a cool down period where any time it
[1028.7s] ran, it would be refusing any form of
[1031.4s] update to the system prompt within 30
[1033.2s] minutes even if I had the review period
[1035.2s] within 5 minutes. So, I basically had
[1037.1s] the Superbase MCP inject its own code to
[1041.0s] allow me to override this. So, I could
[1043.0s] test out different versions or
[1045.0s] permutations of the LLM as a judge
[1047.7s] prompt. Eventually, we broke it down. We
[1049.5s] were good to go. And I saw that if I
[1051.3s] switched and played around with the
[1052.6s] prompt, it was actually working. Now,
[1054.7s] was it working well? Was it being too
[1056.6s] nice to itself? That was the next part.
[1058.6s] Now, when we go to the admin part of the
[1060.7s] app, we now have a very thoughtful set
[1062.4s] of settings where we can set the score
[1064.4s] threshold. We can set how many messages
[1066.8s] to evaluate and whether or not to
[1068.9s] evaluate messages that it's seen before
[1071.2s] or just net new messages. When it first
[1073.5s] came up with this user interface, you
[1075.7s] couldn't see any of this. You could just
[1077.3s] see this reflection interval that only
[1079.6s] had a couple settings like 1 minute, 1
[1082.3s] hour, a couple hours, etc. So all of
[1085.1s] this stuff lives in Superbase, meaning
[1087.4s] the database itself already had the
[1089.3s] functionality. All we had to do is tell
[1091.3s] cloud code, make it come to the four.
[1093.6s] let me see it on the front end so I can
[1095.3s] manipulate it. And key thing here, make
[1097.5s] sure that if I manipulate it on the
[1099.0s] front end, it gets propagated or it gets
[1102.1s] sent that same change to the database.
[1104.6s] Don't just fool me and make it look like
[1106.5s] I'm doing something on the front end
[1107.9s] that isn't actually changing the back
[1109.4s] end. So once we had these two toggles,
[1111.8s] one to evaluate the last n number of
[1113.8s] messages and ideally I could pick what
[1115.6s] those messages are or unevaluated
[1117.6s] messages, we now had a good pipeline for
[1120.1s] reflection. And the next step was
[1122.6s] understanding what if I wanted to
[1124.7s] reflect now like I wanted to run it and
[1127.1s] not wait five minutes or not wait one
[1128.9s] minute. So then we added a reflect now
[1131.1s] button that will override any other
[1133.7s] setting so it re-reflects on whatever
[1135.9s] the number of messages that I set to
[1137.8s] are. And the goal is that it
[1138.9s] automatically checks the system prompt
[1141.0s] ignores the cooldown so it doesn't
[1142.5s] override and generate suggestions or
[1145.0s] evaluates and updates the prompt. And
[1146.6s] you can see this right here. Here if I
[1147.8s] click on reflect now it will go through
[1150.2s] and reflect on the last system prompt
[1152.0s] and the last few messages. In this case
[1153.6s] I have unevalued messages. So there have
[1156.1s] been no new messages. So it won't really
[1158.2s] have any form of real change. And then
[1161.0s] we have the last 14 messages but these
[1163.5s] two are basically going against each
[1165.0s] other. So I just forgot we actually just
[1166.9s] sent a message. So we do have two
[1169.3s] unevaluated messages. It did evaluate
[1171.5s] it. And you can see right here it graded
[1173.5s] itself as perfect. And technically what
[1175.7s] it came back with was pretty good. So
[1177.6s] it's not wrong. But you can see the
[1179.0s] value of being able to tinker and design
[1181.6s] your app so you can test and tinker and
[1183.9s] stress test it and build that and bake
[1186.2s] it into the back end itself. So because
[1187.9s] these sessions would drag on and almost
[1190.0s] always cap out the context limit. And
[1192.0s] you can see right here one example 85%
[1194.6s] used. I would create this handoff
[1197.0s] document that I called the baton pass
[1199.4s] self-improving basically checklist. It
[1201.9s] would go through and maintain context on
[1204.5s] what it did, what bugs it encountered,
[1207.4s] and basically what phase it was in and
[1209.8s] what was completed. It would denote
[1211.4s] something as completed with this check
[1213.0s] mark emoji. And if there was anything
[1215.0s] left or anything to investigate, I could
[1217.8s] always refer to the next chat to go
[1220.2s] through and see what the latest update
[1221.8s] was. So, it's my hacky way of keeping
[1224.4s] the most important pieces of context
[1226.7s] together because yes, you can use things
[1228.6s] like slashcompact to summarize the
[1231.2s] conversation and generate a brand new
[1233.1s] session, but many times some micro
[1235.8s] behaviors or some really pivotal pieces
[1238.2s] of information get left out. And the
[1240.2s] overall goal of this was to create one
[1242.3s] unified save state where everything
[1244.7s] that's changing, every bug that I
[1246.5s] encountered, everything that needed
[1248.6s] investigation could be in one place. So
[1251.3s] I could be the lazy person that I am and
[1253.6s] say refer to and I tag the file at
[1256.7s] handoff and execute on all the remaining
[1259.0s] bugs in the app. And this is a big hack
[1261.0s] for memory management, context
[1263.3s] management, and overall if you ever
[1265.2s] build this project and you want to
[1266.3s] replicate it, it's cool to take this
[1267.9s] artifact as something to your new
[1269.9s] project and take the code, the context,
[1272.6s] and it'll help you build other
[1274.1s] self-improving systems that much faster.
[1276.4s] Now, at this point, we had finished
[1277.9s] quite a few of the phases of the core
[1279.8s] build. So the first pass of the entire
[1282.0s] app was put together. And now this is
[1284.0s] the part where I would go in test things
[1285.8s] and realize I didn't have everything at
[1287.8s] my disposal that I needed to test things
[1289.5s] out. So as a good example, if I would go
[1292.2s] to the prompts, I wanted some way to see
[1294.5s] the system history of all the different
[1296.6s] prompts that I had before and ideally
[1299.1s] revert back to them. We didn't have
[1300.8s] this. It would overwrite the existing
[1302.8s] prompt without any form of history. So I
[1304.9s] wanted that. So then superbase MCP had
[1308.2s] to listen to those requirements, create
[1309.8s] new tables to store that on the
[1312.0s] reflection logs. I would only have the
[1313.8s] last reflection. I wanted all of them.
[1316.2s] And again, this is something that
[1317.8s] existed in the database, but just wasn't
[1319.8s] shown on the front end. So this
[1321.9s] iterative process is something you can
[1324.1s] only understand once you're in it and
[1326.2s] you see what's missing from your initial
[1327.8s] requirements, which is how this
[1329.0s] suggestions tab was born because I
[1331.5s] wanted to know how close were we
[1333.4s] threshold-wise to not passing the test.
[1336.4s] Like what was it that the AI noticed?
[1338.6s] Does the AI know what to look for? And
[1340.7s] is there a way that I can hide these or
[1342.6s] check these off if I've completed them?
[1345.0s] So if I say mark is addressed, I can do
[1347.4s] that. If I'm unhappy with it, I feel
[1349.6s] like it's not a great piece of advice, I
[1352.2s] can just hide it entirely. And those are
[1354.3s] the extra second, third, fourth order
[1357.0s] features that come about when you're
[1358.6s] actually testing out the app. And like I
[1360.2s] said, I wanted to be able to intervene
[1361.9s] and see what is this reflection prompt
[1364.2s] that keeps passing with magnificent
[1366.4s] colors. Cuz like I said before, one of
[1368.7s] the problems was I assumed some
[1370.2s] overconfidence. It was rating itself
[1372.1s] like it was amazing all the time, and I
[1374.6s] felt like it was being too nice to the
[1376.5s] other AI. It's good to be stable, but
[1378.6s] it's not good to be biased. So then I
[1381.0s] created this next tab where I could
[1383.4s] audit and see exactly what the prompt
[1385.9s] was that was being used as the judge for
[1388.2s] the app. You'll notice here, this is an
[1390.2s] example of me trying to do the
[1391.6s] following, which is create the ruthless
[1394.1s] critic so I could purposely break it and
[1396.6s] see whether or not it would respond. So
[1398.5s] you can see here from the very first
[1399.9s] line of the prompt I say you are an
[1402.3s] impossibly critical quality assurance
[1404.6s] system for an AI consultancy chatbot.
[1407.4s] Your standards are unreasonably high.
[1409.8s] You are looking for perfection and
[1411.5s] perfection does not exist. And I
[1413.4s] basically set it up for failure just to
[1415.1s] test out that it would would work. It
[1416.8s] would actually update the prompt by
[1418.7s] making it impossible to score anything
[1420.6s] above a four. So, if I went back to the
[1423.0s] dashboard and I set this to 4.5, no
[1426.1s] matter how good the conversations were,
[1428.5s] it would have to fail if the app was
[1430.5s] actually working. And I know I'm pseudo
[1432.4s] rambling here, but I wanted to walk you
[1434.7s] through the mental model of how to build
[1436.6s] an app and how to make sure you can
[1438.4s] stress test whether it does what it says
[1440.6s] it does. And after this phase of
[1442.2s] creating version control, logging
[1444.4s] everything, this app is not perfect.
[1447.1s] There are still many areas, especially
[1449.0s] if there were active users on the
[1450.6s] platform, where I could see that it
[1452.6s] could fail or would need more
[1453.9s] adjustments. But the cool part is now
[1456.5s] that we have the understanding of a
[1458.0s] self-improving system, you don't just
[1460.0s] have to stop here. It's not just about
[1462.2s] improving the system prompt, you could
[1464.3s] improve the app itself. You could create
[1466.5s] a part of this app that would say based
[1468.7s] on user behavior and what people are
[1470.6s] asking for, maybe come up with and
[1473.6s] implement a new feature or at least
[1475.4s] draft a new feature that we could add to
[1477.5s] this app that's maybe not chat oriented
[1480.2s] or it's an area where you can go back
[1481.9s] and forth and build something like a
[1483.4s] document cuz everyone's asking for XYZ
[1486.5s] document. The sky's is the limit and I
[1488.3s] wanted to show you this example end to
[1490.6s] end at least theoretically and
[1491.8s] conceptually so you can apply it to
[1493.8s] whatever makes sense for you. Now, if
[1495.4s] you enjoyed this video, it took me a
[1496.9s] while to put this mad scientist
[1498.3s] experiment together and break it down in
[1500.7s] a way that I could show you in an easy
[1502.5s] way. Now, if you want to build a version
[1504.0s] of this app on your end, then I'll give
[1506.1s] you the mega prompt I put together in
[1508.5s] the second link in the description below
[1510.4s] along with a guide and a few other
[1512.1s] goodies to help you do that. And if you
[1513.9s] want access to this system asis, carbon
[1516.7s] copy, along with a series of other
[1518.8s] systems that I'm continually building
[1520.6s] for my community, you can check out the
[1522.5s] first link in the description below. and
[1524.1s] I could see you in my early AI adopters
[1525.8s] community. And last but not least, if
[1527.3s] you like videos just like these where I
[1529.0s] go very mad scientist and I try to go
[1531.0s] against the grain and see what's
[1532.4s] possible, please let me know down in the
[1534.2s] comments below. It gives me feedback
[1535.8s] that I should do more of this and that
[1537.4s] you like it. And number two, it helps
[1539.4s] the video and helps the channel. I'll
[1540.7s] see you the next one.
